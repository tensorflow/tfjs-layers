<!--
Copyright 2018 Google LLC

Use of this source code is governed by an MIT-style
license that can be found in the LICENSE file or at
https://opensource.org/licenses/MIT.
=============================================================================
-->

<!doctype html>
<head>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js"></script>
  <script src="../dist/tfjs_layers.debug.js"></script>
</head>

<style>
  .input-div {
    padding: 5px;
    font-family: monospace;
  }
  .input-output-label {
    display: inline-block;
  }

</style>

<body>
  <h1>TensorFlow.js Layers: Sequence-to-Sequence (English-French Translation) Demo</h1>
  <div class="input-div">
    <span class="input-output-label">Input English sentence:</span>
    <input id="englishSentence" value="Go."></input>
  </div>
  <div class="input-div">
    <span class="input-output-label">Output French sentence:</span>
    <input class="input-box" id="frenchSentence" readonly="true"></input>
  </div>
  <div>
    <button class="output-box" id="translate">translate</button>
  </div>

  <script>
    'use strict';

    (async () => {
      const artifactsDir = '../../dist/demo/translation/';

      // Load metadata for translation.
      const translationMetadata =
          await (await fetch(artifactsDir + 'translation.metadata.json')).json();

      const maxDecoderSeqLength = translationMetadata['max_decoder_seq_length'];
      const maxEncoderSeqLength = translationMetadata['max_encoder_seq_length'];
      console.log('maxDecoderSeqLength = ' + maxDecoderSeqLength);
      console.log('maxEncoderSeqLength = ' + maxEncoderSeqLength);
      const inputTokenIndex = translationMetadata['input_token_index'];
      const targetTokenIndex = translationMetadata['target_token_index'];

      const reverseInputCharIndex = _.invert(inputTokenIndex);
      const reverseTargetCharIndex = _.invert(targetTokenIndex);

      console.log('Loading model...');
      const model = await tfjs_layers.loadModel(artifactsDir + 'model.json');
      console.log('Done loading model.');

      const numEncoderTokens = model.input[0].shape[2];
      const numDecoderTokens = model.input[1].shape[2];
      console.log('numEncoderTokens = ' + numEncoderTokens);
      console.log('numDecoderTokens = ' + numDecoderTokens);

      const encoderInputs = model.input[0];
      console.assert(model.layers[2].constructor.name === 'LSTM');
      const stateH = model.layers[2].output[1];
      const stateC = model.layers[2].output[2];
      const encoderStates = [stateH, stateC];

      const encoderModel = tfjs_layers.model({
          inputs: encoderInputs, outputs: encoderStates});

      const latentDim = stateH.shape[stateH.shape.length - 1];
      console.log('latentDim = ' + latentDim);
      const decoderStateInputH = tfjs_layers.input(
          {shape: [latentDim], name: "decoder_state_input_h"});
      const decoderStateInputC = tfjs_layers.input(
          {shape: [latentDim], name: "decoder_state_input_c"});
      const decoderStateInputs = [decoderStateInputH, decoderStateInputC];

      console.assert(model.layers[3].constructor.name === 'LSTM');
      const decoderLSTM = model.layers[3];
      const decoderInputs = decoderLSTM.input[0];
      const applyOutputs = decoderLSTM.apply(
          decoderInputs, {initialState: decoderStateInputs});
      let decoderOutputs = applyOutputs[0];
      const decoderStateH = applyOutputs[1];
      const decoderStateC = applyOutputs[2];
      const decoderStates = [decoderStateH, decoderStateC];

      const decoderDense = model.layers[4];
      console.assert(decoderDense.constructor.name === 'Dense');
      decoderOutputs = decoderDense.apply(decoderOutputs);
      const decoderModel = tfjs_layers.model({
          inputs: [decoderInputs].concat(decoderStateInputs),
          outputs: [decoderOutputs].concat(decoderStates)
      });

      function decodeSequence(inputSeq) {
        // Encode the inputs state vectors.
        let statesValue = encoderModel.predict(inputSeq);

        // Generate empty target sequence of length 1.
        let targetSeq = tfjs_layers.dl.buffer([1, 1, numDecoderTokens]);
        // Populate the first character of the target sequence with the start
        // character.
        targetSeq.set(1, 0, 0, targetTokenIndex['\t']);

        // Sample loop for a batch of sequences.
        // (to simplfy, here we assume that a batch of size 1).
        let stopCondition = false;
        let decodedSentence = '';
        while (!stopCondition) {
          const predictOutputs =
              decoderModel.predict([targetSeq.toTensor()].concat(statesValue));
          const outputTokens = predictOutputs[0];
          const h = predictOutputs[1];
          const c = predictOutputs[2];

          // Sample a token.
          // TODO(cais): Replace the following with tfjs_layers.backend.argmax when
          //   it is available.
          const outputTokenShape = outputTokens.shape;
          const logits = tfjs_layers.backend.sliceAlongFirstAxis(
              tfjs_layers.backend.sliceAlongFirstAxis(
                  outputTokens, 0, 1).reshape([outputTokenShape[1], outputTokenShape[2]]),
              outputTokenShape[1] - 1, 1).dataSync();
          let maxLogit;
          let sampledTokenIndex;
          for (let i = 0; i < logits.length; ++i) {
            if (i === 0 || logits[i] > maxLogit) {
              maxLogit = logits[i];
              sampledTokenIndex = i;
            }
          }
          const sampledChar = reverseTargetCharIndex[sampledTokenIndex];
          decodedSentence += sampledChar;

          // Exit condition: either hit max length or find stop character.
          if (sampledChar === '\n' || decodedSentence.length > maxDecoderSeqLength) {
            stopCondition = true;
          }

          // Update the target sequence (of length 1).
          targetSeq = tfjs_layers.dl.buffer([1, 1, numDecoderTokens]);
          targetSeq.set(1, 0, 0, sampledTokenIndex);

          // Update states.
          statesValue = [h, c];
        }

        return decodedSentence;
      }

      // Encode a string (e.g., a sentence) as a Tensor3D that can be fed directly
      // into the Keras model.
      function encodeString(str) {
        const strLen = str.length;
        const encoded = tfjs_layers.dl.buffer([
            1, maxEncoderSeqLength, numEncoderTokens]);
        for (let i = 0; i < strLen; ++i) {
          if (i >= maxEncoderSeqLength) {
            console.error(
                'Input sentence exceeds maximum encoder sequence length: ' +
                maxEncoderSeqLength);
          }
          const tokenIndex = inputTokenIndex[str[i]];
          if (tokenIndex == null) {
            console.error(
                'Character not found in input token index: "' + tokenIndex + '"');
          }
          console.log('"' + str[i] + '" --> ' + tokenIndex);
          encoded.set(1, 0, i, tokenIndex);
        }
        return encoded.toTensor();
      }

      function doTranslation() {
        const inputSentence = $('#englishSentence').val();
        const inputSeq = encodeString(inputSentence);
        const decodedSentence = decodeSequence(inputSeq);
        $('#frenchSentence').val(decodedSentence);
      }

      $('#translate').click(doTranslation);

      doTranslation();
    })();
  </script>
</body>
